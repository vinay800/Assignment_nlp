{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Assignment:1\n",
    "\n",
    "Understanding textual datasets\n",
    "\n",
    "Download any one of the text dataset mentioned in the previous lecture.\n",
    "\n",
    "Compute tokens, types, and TTR.\n",
    "\n",
    "Plot Zipf’s law and check if Zipf’s law holds true for meanings and lengths.\n",
    "When and when not?\n",
    "\n",
    "Plot Heaps’ law. Fit a curve and report the estimated K and β values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "file = open('shakespeare.txt', 'r')\n",
    "whole_text=file.read()\n",
    "print(type(whole_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_text: 5458199\n"
     ]
    }
   ],
   "source": [
    "len_text=len(whole_text)\n",
    "print('len_text:',len_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Compute Tokens\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "tokens_tmp = word_tokenize(whole_text)\n",
    "tokens=len(tokens_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tokens_tmp))\n",
    "print(\"Tokens are:\",tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compute type\n",
    "import numpy as np\n",
    "type_tmp=np.unique(tokens_tmp)\n",
    "print(\"Type:\",len(type_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compute TTR\n",
    "\n",
    "TTR=len(type_tmp)/tokens\n",
    "print(\"TTR:\",TTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Zipf’s law \n",
    "import re \n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# regualar expression for word segmentation in the whole_text (can use word_tokenize(whole_text) also, but that includes different other things)\n",
    "words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', whole_text)\n",
    " \n",
    "fdist1 = FreqDist(words)\n",
    "    \n",
    "freq = []         \n",
    "for key in fdist1.keys():\n",
    "    freq.append(fdist1[key])\n",
    "    \n",
    "    \n",
    "#print(freq)\n",
    "\n",
    "freq = list((sorted(freq, reverse=True)))\n",
    "rank=range(len(freq))  \n",
    "\n",
    "plt.plot(rank, freq, '.k')    \n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Rank')\n",
    "plt.title(\"Zipf's law\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the words based on ranking and frequency\n",
    "\n",
    "from operator import itemgetter \n",
    "# Uncomment the following two lines to see the word and corresponding frequency in decreasing order\n",
    "#for key, value in sorted(fdist1.items(), key = itemgetter(1), reverse=True):\n",
    "#    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  check if Zipf’s law holds true for meanings\n",
    "# First randomly select some words (no_words) and their frequencies\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "#stored_list=sorted(fdist1.items(), key = itemgetter(1), reverse=True)\n",
    "stored_list= list(fdist1.items())\n",
    "\n",
    "stored_list = np.array(stored_list)\n",
    "np.random.seed(3)\n",
    "import random\n",
    "no_words=10\n",
    "random_list_idx = random.sample(range(stored_list.shape[0]), no_words)\n",
    "  \n",
    "selected_lists=list(stored_list[random_list_idx])\n",
    "#print(selected_lists)\n",
    "#print(random_list_idx)\n",
    "selected_lists = list(selected_lists)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_word=[]\n",
    "actual_word_freq=[]\n",
    "for w in selected_lists:\n",
    "    actual_word.append(w[0])\n",
    "    actual_word_freq.append(w[1])\n",
    "    \n",
    "#print(actual_word)\n",
    "#print(actual_word_freq)\n",
    "len(actual_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_len=[] # this will define meaning\n",
    "synonyms=[]\n",
    "for w1 in actual_word:\n",
    "    tmp_syn = []\n",
    "    for syn in wn.synsets(w1):\n",
    "        for l in syn.lemmas():\n",
    "            tmp_syn.append(l.name())\n",
    "    synonyms.append(tmp_syn)\n",
    "    synonyms_len.append(len(set(tmp_syn)))\n",
    "\n",
    "\n",
    "#print(synonyms)     \n",
    "#print(synonyms_len)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actual_word[0])\n",
    "print(wn.synsets(actual_word[0]))\n",
    "print(set(synonyms[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(actual_word_freq, synonyms_len, '.r')    \n",
    "plt.ylabel('Meaning')\n",
    "plt.xlabel('Frequency')\n",
    "plt.title(\"Zipf's law another form\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zipf’s law for lengths\n",
    "freq=[]\n",
    "word_len=[]\n",
    "for key, value in sorted(fdist1.items(), key = itemgetter(1), reverse=True):\n",
    "    freq.append(value)\n",
    "    word_len.append(len(key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(word_len, freq, '.k')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Length')\n",
    "plt.title(\"Zipf's law another form\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Heaps’ law. Fit a curve and report the estimated K and β values.\n",
    "How does the size of the overall vocabulary (number of unique words) grow\n",
    "with the size of the corpus?\n",
    "Heaps’ Law\n",
    "Let |V| be the size of vocabulary and N be the number of tokens.\n",
    "\n",
    "|V| = KN^β\n",
    "\n",
    "Typically\n",
    "K ≈ 10-100\n",
    "β ≈ 0.4 - 0.6 (roughly square root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize_list = sent_tokenize(whole_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sent_tokenize_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_tokenize_list\n",
    "#np.unique(np.array(word_tokenize(sent_tokenize_list[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = []\n",
    "total_words = []\n",
    "for sentenance in sent_tokenize_list:\n",
    "    tokens_words = word_tokenize(sentenance)\n",
    "    unique_words.append(np.unique(np.array(tokens_words)).shape[0])\n",
    "    total_words.append(len(tokens_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_words, unique_words, '.k')\n",
    "plt.ylabel('Vocabulary(|V|)')\n",
    "plt.xlabel('N')\n",
    "plt.title(\"Heaps’ law\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating K and beta\n",
    "# Reference: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html\n",
    "total_words = np.array(total_words)\n",
    "unique_words = np.array(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, k, b):\n",
    "    return k * (x**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt, pcov = curve_fit(func, total_words, unique_words,bounds=([0., 0.2], [100., 1]))\n",
    "popt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_words, unique_words, '.k', label = 'Actual')\n",
    "plt.plot(total_words, func(total_words, *popt), '.r',\n",
    "         label='Estimated: k=%f, b=%f' % tuple(popt))\n",
    "\n",
    "plt.ylabel('Vocabulary(|V|)')\n",
    "plt.xlabel('N')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
