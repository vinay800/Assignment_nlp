{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Modeling and Smoothing\n",
    "1. Download any of these text books from Project Gutenberg\n",
    "a. Alice in Wonderland: ​ ​Alice’s Adventures in Wonderland\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "url=\"http://www.gutenberg.org/files/11/11-0.txt\" # url for downloading \"alice_in_wonderland.txt\"\n",
    "response = request.urlopen(url)\n",
    "whole_text = response.read().decode('utf8')\n",
    "# Now Compute Tokens\n",
    "#from nltk import word_tokenize\n",
    "#tokens = word_tokenize(whole_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Parse the dataset into sentences using sentence ​tokenizer ​ and divide it into 80/20 ratio. Keep\n",
    "80% dataset for training N-grams and keep 20% for test. You can filter out unnecessary symbols,\n",
    "newlines, etc. You can add symbols <s> and </s> to mark sentence start and end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Total Sentences: 1093\n",
      "Length of train_samples sentences: 874\n",
      "Length of test_samples sentences: 219\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenized_list = sent_tokenize(whole_text)\n",
    "print('Length of Total Sentences:',len(sent_tokenized_list))\n",
    "import random\n",
    "import math\n",
    "\n",
    "random.shuffle(sent_tokenized_list) # randomly suffle the list\n",
    "train_percentage=80\n",
    "test_percentage=20\n",
    "\n",
    "no_of_training_sample=math.floor((train_percentage*len(sent_tokenized_list))/100)\n",
    "train_samples=sent_tokenized_list[:no_of_training_sample] # This variable store the 80% of the training sentences\n",
    "test_samples=sent_tokenized_list[no_of_training_sample:]# This variable store the 20% of the training sentences\n",
    "print('Length of train_samples sentences:',len(train_samples))\n",
    "print('Length of test_samples sentences:',len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‘Stolen!’ the King exclaimed, turning to the jury, who instantly made a\\r\\nmemorandum of the fact.',\n",
       " '‘Oh, you can’t help that,’ said the Cat: ‘we’re all mad here.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[2:4] # print some sample train samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add start and end symbols to the train and test samples (same thing can be done on 'sent_tokenized_list')\n",
    "for num in range(len(train_samples)):\n",
    "    train_samples[num]='<s> '+train_samples[num]+' </s>'\n",
    "    \n",
    "for num in range(len(test_samples)):\n",
    "    test_samples[num]='<s> '+test_samples[num]+' </s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> ‘Stolen!’ the King exclaimed, turning to the jury, who instantly made a\\r\\nmemorandum of the fact. </s>',\n",
       " '<s> ‘Oh, you can’t help that,’ said the Cat: ‘we’re all mad here. </s>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-Compute MLE for unigram, bigram, trigrams and quadgrams. How many n-grams are possible\n",
    "and how many actually exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 29465\n",
      "Type: 6019\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "# Now Compute Tokens for checking purpose\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "tokens_tmp =whole_text.split()\n",
    "\n",
    "print('Tokens:',len(tokens_tmp))\n",
    "\n",
    "# Now compute type\n",
    "import numpy as np\n",
    "type_tmp=np.unique(tokens_tmp)\n",
    "print(\"Type:\",len(type_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible unigrams =6019\n",
      "Total unigrams =6019\n"
     ]
    }
   ],
   "source": [
    "# MLE for unigram \n",
    "from nltk.probability import FreqDist\n",
    "output = list(ngrams(whole_text.split(), 1))\n",
    "fdist1 = FreqDist(output)\n",
    "# Uncomment the following two lines to print the probablities\n",
    "#for item in fdist1.keys():\n",
    "#    print('Prob of', item, 'is', fdist1[item]/len(fdist1))\n",
    "\n",
    "# mle_unigram is a dictionary, which stores n-grams and corresponding probablities\n",
    "mle_unigram={}\n",
    "for key, value in fdist1.items():\n",
    "    mle_unigram[key]=fdist1[key]/len(output)\n",
    "    \n",
    "print(\"Possible unigrams ={}\".format(len(type_tmp))) #type_tmp is vocabulary size\n",
    "print(\"Total unigrams ={}\".format(len(mle_unigram)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible bigrams =36228361\n",
      "Total bigrams =19419\n"
     ]
    }
   ],
   "source": [
    "# MLE for bigram \n",
    "output = list(ngrams(whole_text.split(), 2))\n",
    "fdist1 = FreqDist(output)\n",
    "# Uncomment the following two lines to print the probablities\n",
    "#for item in fdist1.keys():\n",
    "#    print('Prob of', item, 'is', fdist1[item]/len(fdist1))\n",
    "\n",
    "# mle_bigram is a dictionary, which stores n-grams and corresponding probablities\n",
    "mle_bigram={}\n",
    "for key, value in fdist1.items():\n",
    "    mle_bigram[key]=fdist1[key]/len(output)\n",
    "    \n",
    "print(\"Possible bigrams ={}\".format(len(type_tmp)**2))\n",
    "print(\"Total bigrams ={}\".format(len(mle_bigram)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible trigrams =218058504859\n",
      "Total trigrams =26595\n"
     ]
    }
   ],
   "source": [
    "# MLE for trigram \n",
    "output = list(ngrams(whole_text.split(), 3))\n",
    "fdist1 = FreqDist(output)\n",
    "# Uncomment the following two lines to print the probablities\n",
    "#for item in fdist1.keys():\n",
    "#    print('Prob of', item, 'is', fdist1[item]/len(fdist1))\n",
    "    \n",
    "# mle_trigram is a dictionary, which stores n-grams and corresponding probablities    \n",
    "mle_trigram={}\n",
    "for key, value in fdist1.items():\n",
    "    mle_trigram[key]=fdist1[key]/len(output)\n",
    "    \n",
    "    \n",
    "print(\"Possible trigrams ={}\".format(len(type_tmp)**3))\n",
    "print(\"Total trigrams ={}\".format(len(mle_trigram)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible quadgrams =1312494140746321\n",
      "Total quadgrams =28552\n"
     ]
    }
   ],
   "source": [
    "# MLE for quadgram \n",
    "output = list(ngrams(whole_text.split(), 4))\n",
    "fdist1 = FreqDist(output)\n",
    "# Uncomment the following two lines to print the probablities\n",
    "#for item in fdist1.keys():\n",
    "#    print('Prob of', item, 'is', fdist1[item]/len(fdist1))\n",
    "\n",
    "# mle_quadgram is a dictionary, which stores n-grams and corresponding probablities    \n",
    "mle_quadgram={}\n",
    "for key, value in fdist1.items():\n",
    "    mle_quadgram[key]=fdist1[key]/len(fdist1)\n",
    "\n",
    "    \n",
    "print(\"Possible quadgrams ={}\".format(len(type_tmp)**4))\n",
    "print(\"Total quadgrams ={}\".format(len(mle_quadgram)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- Develop a system that has two functions:\n",
    "\n",
    "a. Generator(model_name): generates sentences by utilizing MLEs from specified\n",
    "n-gram model. Sampling from multinomial distribution can be done using a\n",
    "predefined ​function ​. Note, 5-10 sentences would suffice for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accepts integer values as the input argument(for example model_name=1 corresponds to unigram)\n",
    "# model_name=n, in general generates sentences using n-gram model(n=2,3,4....)\n",
    "# And generates sentences by utilizing MLEs from specified n-gram model using the corpus whole_text\n",
    "\n",
    "# Size is the optional argument which defines number of n-grams to choose for sentence formation and can be \n",
    "# chaged to generate desired sentences having specific number of n-grams\n",
    "\n",
    "# Assumption:whole_text variable defined earlier in this notebook is availble and that is the case if notebook is \n",
    "# run sequentially\n",
    "def Generator(model_name,size=150):\n",
    "    random.seed(3)\n",
    "    output = list(ngrams(whole_text.split(), model_name))\n",
    "    fdist1 = FreqDist(output)\n",
    "    mle_ngram={}\n",
    "    for key, value in fdist1.items():\n",
    "        mle_ngram[key]=fdist1[key]/len(output)\n",
    "\n",
    "    \n",
    "    ngram_words=[]\n",
    "    ngram_prob=[]\n",
    "    for key, value in mle_ngram.items():\n",
    "        ngram_words.append(key) \n",
    "        ngram_prob.append(value)\n",
    "    # function page:https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.multinomial.html\n",
    "    # One can play with 'Number of experiments' that is taken 20 here\n",
    "    word_idx_one_hot=np.random.multinomial(20, ngram_prob, size)\n",
    "    normal_idx=np.argmax(word_idx_one_hot,axis=-1)\n",
    "    sentences = ''\n",
    "    if model_name == 1:\n",
    "        for i in normal_idx:\n",
    "            sentences += ' '.join(ngram_words[i])+ ' '\n",
    "        print (sentences)\n",
    "    else:\n",
    "        for i in normal_idx:\n",
    "            sentences +=  ' '.join(ngram_words[i])+ ' '\n",
    "        print (sentences)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said she could the of surprised ornamented she herself and said the but a the to it a things the she and heard the a a the and a a the had said the to her and all she she the she very ‘Poor angrily and ever mean wanted her the the she to it I you large of she to the and ‘I the a of a the and the the once; must must she United she to to she the herself, she White the whiting the to the she the the fact stood her again, she her I is to afraid have the a to any her the to she you the said, the she the and she stood jury the the out, and herself must and the Turtle of her Dormouse; said to the her general I have head! the it as silence. silence. the found with \n"
     ]
    }
   ],
   "source": [
    "# Examples of using the function 'Generator(model_name,size=150)'\n",
    "# Use unigram model with 150 unigrams \n",
    "Generator(1,size=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up and for a they’re not,’ might just Domain in the King, remarked; ‘they’d others. ‘We and expenses, one end thoughtfully at its undoing quite a to one now.’ CHAPTER splashing paint out to fee of She’ll get was mouth pulled out support. Project where it close by thought Alice. it is without hearing but frowning you doing think to the same chorus of did not for the a library might just the pebbles She gave up and in the you butter sight, they getting so it what didn’t sound Duchess said already heard the Mock liked, so cut off, When I certain it Alice could refreshments!’ But with the didn’t sound more of of living with the she was the King, going through not,’ Alice found her wouldn’t say Once more it was for a she squeezed have none, just in were trying--’ was exactly spoke for tea-tray in the Cat. to see this time). spread out on the tossing his the same a Project you’re a but, after these efforts, the requirements if there sharp bark they had on the who was with the a candle. the same two feet and stupid), book,’ thought did not the King, works even the table: about it. Owl and she thought, was pressed donate, please defective or agreement. If found that nonsense!’ said round and presents like and repeated explanation; ‘I’ve is the verdict,’ the said, ‘And do:-- ‘How and unlocking ‘em together and was don’t believe for some Will you, in her by sending and was court with voice of was looking eagerly that seemed to it chuckled. uncomfortable for perhaps I was pressed the Conqueror, Alice. ‘I’m for asking! it in with the ran off away, comfortably to know. is the to be on their would happen take more.’ \n"
     ]
    }
   ],
   "source": [
    "# Examples of using the function 'Generator(model_name,size=150)'\n",
    "# Use unigram model with 150 bigrams \n",
    "Generator(2,size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dodged behind a with the Mouse and you’ll understand it say, as air, ‘are you that she might THAT YOU HAVE over!’ thought Alice. further off from a simple question,’ whether it would of them. However, *** ***** This and, after folding and without paying case I can the e--e--evening, Beautiful, had been anything then treading on heavy sobbing of Lewis Carroll *** humbly: ‘you had as himself, and remember where.’ ‘Well, was.’ ‘I never say you never you’re changed, do of course, I ‘I couldn’t afford it was all ‘as I mentioned on the top donation methods and she spoke. (The minute there was no restrictions whatsoever. a queer thing, about her, to was VERY ugly; see after some Hatter. ‘I deny ‘Consider your verdict,’ in any way down the chimney, know who I tone. And she Gutenberg-tm electronic works, out of sight; go down the hoarse growl, ‘the first speech. ‘You such thing!’ Alice the thing yourself, quite faint in mind, she turned IS the use by the hand, dear Sir, With we do not talking!’ Just then curious appearance in just as I’d to listen. ‘Mary hot, she kept gave me a said Alice; ‘all Then again--“BEFORE SHE tongue hanging out and she hastily I’m not Ada,’ tried to curtsey and was just to her, though, Five! Don’t go said Alice; ‘I Mock Turtle interrupted, shriek, ‘and just tea-tray in the had nothing else of saying to require such a CHAPTER II. The animal’s feelings. ‘I the e--e--evening, Beautiful, of them even don’t believe you other guests had frog; and both young lady tells them,’ said the have to beat looking thoughtfully at ‘What are tarts all other terms ‘I haven’t the of my own. the Dormouse into the air, and had nothing else federal laws and the subject. ‘Go anything but sit UTF-8 *** START thought, and it Dormouse again took down on the way off, and head would go slowly followed her said the Mock And he added moment the door all turning into you doing out it would be frowning like a terms with him, still where she kindly, but he curving it down which tied up now hastily began and we won’t into hers--she could only changing the will be When heads off?’ shouted at all the Let me see: imposed by the is over!’ and away went Alice it’s a French baby, the shriek went down on some time with Turtle sighed deeply, and then added saying anything more hard as she Which way?’, holding have not received came a little talking!’ Just then witness at all: largest telescope that found it so is this New by the time place around her \n"
     ]
    }
   ],
   "source": [
    "# Examples of using the function 'Generator(model_name,size=150)'\n",
    "# Use trigram model with 150 trigrams \n",
    "Generator(3,size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when I find a said the Cat, ‘a her way into a ‘--for they haven’t got she set to work ‘They all can,’ said ready for your walk!” world! Oh, my dear the pool rippling to King replied. Here the hastily, afraid that she a very poor speaker,’ ‘First witness!’ The first at it again: but (a loud crash)--‘Now, who CHAPTER VIII. The Queen’s idea what a delightful her hands up to the Dormouse go on its face to see was, that she had afraid I don’t know was sent for.’ ‘You stored, may contain “Defects,” the blows hurt it a clean cup,’ interrupted was so full of Alice for some time ‘She boxed the Queen’s donations to the Project be in Bill’s place ‘--but I shall have opening its eyes, ‘Of goose, with the bones then, saying to herself wrote down on their use in knocking,’ said me? They’re dreadfully fond just the case with TO YOU,”’ said Alice. game. CHAPTER IX. The delay would cost them she had asked it the earth. At last month is it?’ he While the Duchess sang upon a Gryphon, lying had just upset the conger-eel, that used to head appeared, and then he went on in tomorrow--’ At this moment March Hare. ‘Exactly so,’ and there stood the there were TWO little door, she walked sadly fact. ‘I keep them or associated in any all the time she way wherever she wanted THAN A MILE HIGH file should be named here till I’m somebody her, though, as they in the wind, and was not even room Hare was said to in the window?’ ‘Sure, hear the Rabbit say when I get it ‘Beautiful Soup, so rich you tell me,’ said it belongs to a of it altogether; but OWNER, AND ANY DISTRIBUTOR would not open any plate with the name time,’ said the Caterpillar; great wonder is, that still in existence; ‘and hurt the poor animal’s version posted on the ‘First witness!’ The first itself out again, so I didn’t!’ interrupted Alice. away under her arm, French lesson-book. The Mouse his eyes very wide it. I suppose you’ll in the morning, just stupid for life to The rabbit-hole went straight Hatter was the only and, turning to Alice, thinking while she ran, an individual Project Gutenberg-tm there were ten of CURTSEYING as you’re falling its nose. The Dormouse just the case with he asked. ‘Begin at ‘Mary Ann! Mary Ann!’ arches are gone from I dare say there to twenty at that must be kind to easy to take MORE no mark on the did not like to Dodo suddenly called out gather about her other down again in a that dark hall, and of the month, and chrysalis--you will some day, ready for your walk!” Alice; ‘I might as I needn’t be afraid or not.’ ‘I’m a appeared. ‘I don’t think Alice. ‘I’ve read that steady as ever; Yet the public domain in inwards, and Alice’s elbow quietly said, just as right height to rest youth,’ said the sage, sort of way, ‘Do ‘A barrowful of WHAT?’ tone; ‘Seven jogged my has the main PG wasn’t much. The Hatter series of short charges and the White Rabbit sat down and began appearing and vanishing so of you to sit one of the cakes, well say that “I say, ‘A barrowful will doesn’t understand English,’ thought by the way, was ‘to go on crying wow! wow!’ While the short speech, they all who it was, even did: there’s no name tone, and added with INCLUDING BUT NOT LIMITED more questions about it, \n"
     ]
    }
   ],
   "source": [
    "# Examples of using the function 'Generator(model_name,size=150)'\n",
    "# Use quadgram model with 150 quadgrams \n",
    "Generator(4,size=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Probability(sentence,model_name): Compute the probability of a given sentence\n",
    "in log-space. Note you can provide any sentence, however, a random sentence\n",
    "will mostly lead to zero probability. The better idea is to take sentences from the\n",
    "corpus itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here n is the model name. whole_text as defined earlier is just passed for the completeness purpose\n",
    "def Probability(sentence,n,whole_text):\n",
    "    #print(sentence)\n",
    "    if n==1:\n",
    "        try:\n",
    "            split_sen = list(ngrams(sentence.split(), 1))\n",
    "           \n",
    "            output = list(ngrams(whole_text.split(), 1))\n",
    "            fdist1 = FreqDist(output)\n",
    "            mle_unigram={}\n",
    "            for key, value in fdist1.items():\n",
    "                mle_unigram[key]=fdist1[key]/len(output)\n",
    "\n",
    "            prob_sent=1\n",
    "            for key in split_sen:\n",
    "                prob_sent *= mle_unigram[key]\n",
    "            prob_sent=np.array(prob_sent)\n",
    "            prob_sent_log = np.log(prob_sent)# grams prob in log space\n",
    "            print('Probability of the given sentence (in log space) using n-grams is: {}'.format(prob_sent_log))\n",
    "        except:\n",
    "            prob_sent_log=0\n",
    "            print('Probability of the given sentence is: {}'.format(prob_sent_log))\n",
    "  \n",
    "    else:\n",
    "        output_n = list(ngrams(whole_text.split(), n))\n",
    "        fdist1_n = FreqDist(output_n)\n",
    "\n",
    "        output_n_1 = list(ngrams(whole_text.split(),n-1))\n",
    "        fdist1_n_1 = FreqDist(output_n_1)\n",
    "\n",
    "        n=n-1\n",
    "        split_sen=sentence.split()\n",
    "        prob_sent = []\n",
    "        try:\n",
    "            for i in range(n,len(split_sen)-1):\n",
    "\n",
    "                nk_key = tuple([split_sen[k] for k in range(i-n,i+1)])\n",
    "                #print(nk_key)\n",
    "                nk_1_key = tuple([split_sen[k] for k in range(i-n,i)])\n",
    "                #print(nk_1_key)\n",
    "\n",
    "                ci = fdist1_n[nk_key]\n",
    "                #print(ci)\n",
    "                cj = fdist1_n_1[nk_1_key]\n",
    "                #print(cj)\n",
    "                prob_sent.append(float(ci) / cj)\n",
    "            prob_sent = np.array(prob_sent)\n",
    "            prob_sent = np.log(prob_sent)# grams prob in log space\n",
    "            prob_sent_all  = np.sum(prob_sent)# sentence prob in log space\n",
    "            print('Probability (in log space) of the n-grams is: ',prob_sent)\n",
    "            print('Probability of the given sentence (in log space) using n-grams is: {}'.format(prob_sent_all))\n",
    "\n",
    "        except:\n",
    "            prob_sent_all=0\n",
    "            print('Probability of the whole sentence is: ',prob_sent_all)\n",
    "        \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of the given sentence (in log space) using n-grams is: -229.8932821768834\n"
     ]
    }
   ],
   "source": [
    "#sentence='''She took down a jar from one of the shelves '''\n",
    "sentence='''There was nothing so VERY remarkable in that; nor did Alice think it so\n",
    "VERY much out of the way to hear the Rabbit say to itself, ‘Oh dear!\n",
    "Oh dear! I shall be late!’'''\n",
    "Probability(sentence,1,whole_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-Implement add-1 smoothing for bigram model and give 2-3 examples where drastic change in\n",
    "the count occurs post-smoothing. Can you explain this drastic change in a sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of a sentence After Smoothing 2.06608922172e-131\n",
      "Bigram Count Before Smoothing: [ 14   4   1   5   1   1   1   1   1   1   1   3   3   5   1   1  36 152\n",
      "   8   4   5   5  15   1   3   2   1   1   1   2   4  12   4   1]\n",
      "Effective Count After Smoothing: [0.010683036017092857, 0.05521245888433913, 0.0014921829958964969, 0.021103182386959317, 0.0008141941174475014, 0.00013574507075711812, 0.026853277974954796, 6.787483879725786e-05, 0.00013574507075711812, 0.0033881077418261903, 0.01488917334770599, 0.005016609043454681, 0.04775158445390832, 0.021103182386959317, 0.0008141941174475014, 0.0027790957771300752, 0.12140585887287735, 3.033432021556169, 0.48109479906196795, 0.006609273318872017, 0.15338316026192209, 0.0030529172320217096, 0.8552796427768319, 0.001966501661354852, 0.0047457627118644066, 0.07669158013096104, 0.000271471716040585, 0.00013574507075711812, 0.0006107077424170455, 0.0005089921954530031, 0.0015267693560426139, 0.11370899915895712, 0.0038998914812805206, 0.01039873054458287]\n"
     ]
    }
   ],
   "source": [
    "# Implemetation of add-1 smoothing for bigram model \n",
    "output_uni = list(ngrams(whole_text.split(), 1))\n",
    "fdist1_uni = FreqDist(output_uni)\n",
    "\n",
    "output_bi = list(ngrams(whole_text.split(), 2))\n",
    "fdist1_bi = FreqDist(output_bi)\n",
    "\n",
    "mod_v=len(list(ngrams(whole_text.split(), 1)))\n",
    "\n",
    "\n",
    "##### Three Sentences are given for seeing the results as asked in question. Some other test_sentences can also be given\n",
    "#test_sentence='''She took down a jar from one of the shelves as she passed'''\n",
    "#test_sentence='''She took down a jar from one of the roads as she passed'''\n",
    "test_sentence='''There was nothing so VERY remarkable in that; nor did Alice think it so\n",
    "VERY much out of the way to hear the Rabbit say to itself, ‘Oh dear!\n",
    "Oh dear! I shall be late!’'''\n",
    "\n",
    "\n",
    "#output = list(ngrams(test_sentence.split(), 1))\n",
    "output=test_sentence.split()\n",
    "\n",
    "pws = [] # for probability of a sentence\n",
    "bigram_count=[]\n",
    "c_star=[] # Effective Count after smoothing\n",
    "n= 1 # n=1 for bigram\n",
    "for i in range(n,len(output)):\n",
    "    \n",
    "    nk_key = tuple([output[k] for k in range(i-n,i+1)])\n",
    "    #print(nk_key)\n",
    "    nk_1_key = tuple([output[k] for k in range(i-n,i)])\n",
    "    #print(nk_1_key)\n",
    "    \n",
    "    ci = fdist1_bi[nk_key]\n",
    "    bigram_count.append(ci)\n",
    "    #print(ci)\n",
    "    cj = fdist1_uni[nk_1_key]\n",
    "    #print(cj)\n",
    "    \n",
    "    p_mle=(float(ci)+1) / (cj+mod_v)\n",
    "    pws.append(p_mle)\n",
    "    \n",
    "   \n",
    "    c_star.append(float(p_mle*cj))\n",
    "    \n",
    "\n",
    "pws = np.array(pws)    \n",
    "pws_final  = np.prod(pws)\n",
    "\n",
    "print('probability of a sentence After Smoothing',pws_final)\n",
    "bigram_count=np.array(bigram_count)\n",
    "print(\"Bigram Count Before Smoothing:\", bigram_count)\n",
    "print(\"Effective Count After Smoothing:\", c_star)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques: Drastic change in count?\n",
    "\n",
    "Ans: In add-1 smoothing, unseen n-grams are given very high weights which results in the drastic change in count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6- Do you observe the constant discounting value ‘d’ by implementing Good-turing smoothing\n",
    "technique? If yes, what is the value of ‘d’?\n",
    "Hint: ​You can check for n-grams having original counts between 1-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "53\n",
      "[ 1  2  3  4  5  6  7  8  9 10]\n",
      "C_star:  [0.22292598112270243, 1.0629526462395542, 1.4528301886792452, 4.1341991341991342, 3.424083769633508, 4.5596330275229358, 6.3098591549295771, 5.4642857142857144, 7.3529411764705879]\n",
      "P_star: [7.5657892795758505e-06, 3.6075094051910882e-05, 4.930698077988275e-05, 0.00014030881161374966, 0.00011620851076305813, 0.00015474743008732177, 0.00021414760410417706, 0.00018545004969576495, 0.00024954831754524307]\n",
      "d=c-c*: [ 0.77707402  0.93704735  1.54716981 -0.13419913  1.57591623  1.44036697\n",
      "  0.69014085  2.53571429  1.64705882]\n"
     ]
    }
   ],
   "source": [
    "# Good-turing smoothing technique\n",
    "n=2\n",
    "output_n = list(ngrams(whole_text.split(),n ))\n",
    "fdist1_n = FreqDist(output_n)\n",
    "\n",
    "# Compute Frequency of Frequency N_c\n",
    "freq_freq=FreqDist(fdist1_n.values())\n",
    "c=np.array(list(freq_freq.keys()))\n",
    "print(len(c))\n",
    "N_c=np.array(list(freq_freq.values()))\n",
    "print(len(N_c))\n",
    "\n",
    "\n",
    "c=c[:10]\n",
    "N_c=N_c[:10]\n",
    "\n",
    "N=len(list(ngrams(whole_text.split(), 1)))\n",
    "\n",
    "print(c)\n",
    "\n",
    "\n",
    "c_star=[] #For Effective Count\n",
    "P_star=[]#For Effective Probability\n",
    "for i in range(len(c)-1):\n",
    "    #print(i)\n",
    "    tmp=((c[i]+1)*N_c[i+1])/N_c[i]\n",
    "    c_star.append(tmp)\n",
    "    P_star.append(tmp/N)\n",
    "print('C_star: ', c_star)\n",
    "print('P_star:',P_star)\n",
    "print('d=c-c*:',np.array(c[:9])-np.array(c_star))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the current corpus for the for n-grams having original counts between 1-10, in most cases the value of d is close to 1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7-Compute the perplexity value for the test dataset for the bigram model using add-1 and\n",
    "Good-turing. Which performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(test_dataset, model_name, smoothing):\n",
    "    split_sen = test_dataset.split()\n",
    "    output_n = list(ngrams(whole_text.split(), model_name))\n",
    "    fdist1_n = FreqDist(output_n)\n",
    "\n",
    "    output_n_1 = list(ngrams(whole_text.split(),model_name-1))\n",
    "    fdist1_n_1 = FreqDist(output_n_1)\n",
    "\n",
    "    n=model_name-1\n",
    "    if smoothing=='add-1':\n",
    "        mod_v=len(list(ngrams(whole_text.split(), 1)))\n",
    "        pws = []\n",
    "        for i in range(n,len(split_sen)-1):\n",
    "\n",
    "            nk_key = tuple([split_sen[k] for k in range(i-n,i+1)])\n",
    "            #print(nk_key)\n",
    "            nk_1_key = tuple([split_sen[k] for k in range(i-n,i)])\n",
    "            #print(nk_1_key)\n",
    "\n",
    "            ci = fdist1_n[nk_key]\n",
    "            #print(ci)\n",
    "            cj = fdist1_n_1[nk_1_key]\n",
    "            #print(cj)\n",
    "            pws.append(1/(float(ci)+1) / (cj+mod_v))\n",
    "    elif smoothing=='gt':\n",
    "        # Good-turing smoothing technique\n",
    "        pws = []\n",
    "        for i in range(n,len(split_sen)-1):\n",
    "\n",
    "            nk_key = tuple([split_sen[k] for k in range(i-n,i+1)])\n",
    "            #print(nk_key)\n",
    "            nk_1_key = tuple([split_sen[k] for k in range(i-n,i)])\n",
    "            #print(nk_1_key)\n",
    "            \n",
    "            ci = fdist1_n[nk_key]\n",
    "            #print(ci)\n",
    "            cj = fdist1_n_1[nk_1_key]\n",
    "            #print(cj)\n",
    "            \n",
    "            pws.append(1/(float(ci)-1.6) / (cj)) # subtract the discounting value computed from 'GT (Good Turing)'\n",
    "\n",
    "        \n",
    "\n",
    "    pws = np.array(pws)\n",
    "    print(pws)\n",
    "    pws = np.prod(pws)# grams prob in log space\n",
    "    N=len(list(ngrams(whole_text.split(),1)))\n",
    "    perplexity = np.power(pws, 1/float(N))\n",
    "    print('Perplexity of the sentence with {} smoothing is: {}' .format(smoothing,perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -5.05050505e-02  -7.24637681e-02   5.41125541e-03  -2.51762336e-03\n",
      "  -8.33333333e-01   9.25925926e-03   6.60851176e-04   1.11559332e-05\n",
      "  -1.00160256e-03  -1.66666667e+00   7.61243567e-05]\n",
      "Perplexity of the sentence with gt smoothing is: 0.9980916752322215\n"
     ]
    }
   ],
   "source": [
    "test_dataset='''She took down a jar from one of the shelves as she passed'''\n",
    "model_name=2 # integer value >=2 (n=2 for bigram,n=3 for trigram, so on )\n",
    "smoothing='gt'\n",
    "perplexity(test_dataset, model_name, smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.69503017e-05   1.69560499e-05   6.77002234e-06   1.65964085e-05\n",
      "   1.69681338e-05   6.77736361e-06   1.53858936e-06   2.17422831e-07\n",
      "   1.60621928e-05   1.69687097e-05   6.01027998e-07]\n",
      "Perplexity of the sentence with add-1 smoothing is: 0.9955017677299063\n"
     ]
    }
   ],
   "source": [
    "test_dataset='''She took down a jar from one of the shelves as she passed'''\n",
    "model_name=2 # integer value >=2 (n=2 for bigram,n=3 for trigram, so on )\n",
    "smoothing='add-1'\n",
    "perplexity(test_dataset, model_name, smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the current corpus for the for bigram model Perplexity of the sentence with gt smoothing is approximately equal to the Perplexity of the sentence with add-1 smoothing.\n",
    "So there is no conclusive evidence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
